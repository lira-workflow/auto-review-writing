import gc, os, re
import pandas as pd
from tqdm import tqdm
from typing import Union
from argparse import Namespace
from joblib import Parallel, delayed
from utils.pbar_utils import tqdm_joblib
from baseline_and_mass.prompts import BASELINE_PROMPT
from baseline_and_mass.utils.prompting import prompt_llm
from baseline_and_mass.utils.text_utils import clean_review
from baseline_and_mass.utils.file_utils import (
    load_json,
    save_to_json,
    save_to_file,
    pattern_selector,
    check_paper_in_subset,
)


def parse_review_content(paper_id: Union[str, int], content: str):
    """
    Parses the content generated by `generate_baseline` to extract the title, abstract,
    section headers, and text.

    Parameters:
    paper_id - the paper_id for saving.
    content - the full text content to parse.

    Returns:
    A dictionary with these keys: 'paper_id', 'title', 'abstract', 'section', 'text'.
    """

    # Remove formatting
    content = re.sub("\*+", "", content)

    # Find the titles and abstracts
    title_match = re.search(r"Title:\s*(.+)", content)
    title = title_match.group(1).strip() if title_match else ""

    # Extract abstract
    abstract_match = re.search(r"Abstract:\s*(.+)", content)
    abstract = abstract_match.group(1).strip() if abstract_match else ""

    # Extract sections and their corresponding text
    sections, texts = [], []
    section_pattern = r"Section \d+:\s*(.+)"
    matches = list(re.finditer(section_pattern, content))

    for i, match in enumerate(matches):

        section_name = match.group(1).strip()
        sections.append(section_name)

        # Capture text until the next section header or end of content
        start_index = match.end()
        end_index = matches[i + 1].start() if i + 1 < len(matches) else len(content)
        texts.append(content[start_index:end_index].strip())

    # Extract conclusion
    conclusion_match = re.search(r"Conclusion:\s*(.+)", content)
    conclusion = conclusion_match.group(1).strip() if conclusion_match else ""
    sections.append("Conclusion")
    texts.append(conclusion.strip())

    # Memory Leak prevention
    del matches, content
    gc.collect()
    return {
        "paper_id": int(paper_id) if paper_id.isdigit() else paper_id.split("_")[0],
        "title": title,
        "abstract": abstract,
        "section": sections,
        "text": texts,
    }


def generate_baseline(input_file: str, args: Namespace) -> dict:
    """
    Generates a systematic literature review based on a `_references.txt` file.

    Parameters:
    input_file - the `structured_references` file to read from.
    args - a Namespace containing the following variables which get read:

    dataset - the dataset to use.
    llm - the LLM to use.
    model - the (OpenAI) chat model name to use.
    temperature - the response temperature for randomness control.

    Returns:
    A dictionary with keys: 'paper_id', 'title', 'abstract', 'section', 'text'.

    """
    try:
        with open(input_file, "r", encoding="utf-8") as file:
            content = file.read()

        # Get the paper ID
        pattern = pattern_selector(args.dataset, "_references", use_group=True)
        paper_id = re.search(pattern, os.path.basename(input_file)).group(1)

        # Prompt the LLM
        prompt_content = BASELINE_PROMPT.format(content=content)
        review_content = prompt_llm(prompt_content, input_file, BASELINE_PROMPT, args)

        # Parse the paper content
        parsed_content = parse_review_content(paper_id, review_content)

        # Save to JSON file
        filename = os.path.basename(input_file)
        filename = os.path.join(args.final_papers_dir, filename)

        # For double checking: Save the "raw" output
        debug_name = re.sub("_references", "", filename)
        cleaned_content = clean_review(review_content)
        save_to_file(cleaned_content, debug_name)

        out_name = re.sub("_references.txt", "_papers.json", filename)
        os.makedirs(args.final_papers_dir, exist_ok=True)
        save_to_json(parsed_content, out_name)

        return parsed_content

    except Exception as e:
        print(f"An error occurred when processing `{input_file}`: {e}")
        return None


def generate_all_baselines(args: Namespace):
    """
    Processes `_references.txt` files, generates reviews, and saves them to a CSV file.

    Parameters:
    args - a Namespace containing the following variables which get read:

    dataset - the dataset to use.
    processed_dir - the directory containing the sample `_references.txt` files.
    final_papers_dir - the directory to save the generated reviews.
    final_papers_file - the path of the target CSV file to save.
    n_samples - the number of samples used for selection.
    seed - the seed used for sample selection.
    model - the (OpenAI) chat model name to use.
    """

    # Ensuring the input and output folder exists
    if not os.path.exists(args.processed_dir):
        raise FileNotFoundError(f"Input folder `{args.processed_dir}` does not exist!")

    # Make the final papers folder
    os.makedirs(args.final_papers_dir, exist_ok=True)

    # Load a buffer list to hold all parsed data and get the already
    # generated articles (in case something happens with the API or something else)
    buffer_list, data = [], []
    for filename in os.listdir(args.final_papers_dir):

        pattern = pattern_selector(args.dataset, "_papers", extension="json")
        match = re.search(pattern, filename)
        if match:
            # Check the filenames and load the contents to prevent re-prompting
            buffer_file = os.path.join(args.final_papers_dir, match.group(0))
            buffer_content = load_json(buffer_file)
            buffer_list.append(match.group(0))
            data.append(buffer_content)

    # Processing each file
    file_list = os.listdir(args.processed_dir)

    if args.dataset == "srg":
        file_list = check_paper_in_subset(
            os.listdir(args.processed_dir), args.n_samples, args.seed
        )

    input_files = [
        os.path.join(args.processed_dir, filename)
        for filename in file_list
        if filename.endswith("_references.txt")
        and re.sub("_references.txt", "_papers.json", filename) not in buffer_list
    ]

    # Prompting
    with tqdm_joblib(
        tqdm(
            desc="Generating Baseline Reviews",
            unit="file(s)",
            total=len(input_files),
        )
    ):
        results = Parallel(args.n_jobs, verbose=0)(
            delayed(generate_baseline)(file, args) for file in input_files
        )

    # Combine both results
    data_generated = [result for result in results if result]
    data.extend(data_generated)

    pd.DataFrame(data).to_csv(args.final_papers_file, index=False, encoding="utf-8")
    print(f"Saved reviews to `{args.final_papers_file}`! \n")

    # Memory Leak prevention by deleting variables
    del data, file_list, buffer_list, data_generated
    gc.collect()
