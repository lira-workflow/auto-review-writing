import os, re, copy, time, threading
from tqdm import tqdm
from typing import Dict, List, Tuple, Union, Optional
from autosurvey.src.model import APIModel
from autosurvey.src.database import database
from autosurvey.src.constants import RAW_SAVEDIR, REFINED_SAVEDIR
from autosurvey.src.utils import (
    load_txt,
    load_json,
    save_to_json,
    tokenCounter,
)
from autosurvey.src.prompt import (
    LCE_PROMPT,
    CHECK_CITATION_PROMPT,
    SUBSECTION_WRITING_PROMPT,
)


class subsectionWriter:

    def __init__(self, model: str, api_key: str, api_url: str, database: database):
        self.model, self.api_key, self.api_url = model, api_key, api_url
        self.api_model = APIModel(self.model, self.api_key, self.api_url)

        self.db = database
        self.token_counter = tokenCounter()
        self.input_token_usage, self.output_token_usage = 0, 0

    def reset_token_usage(self):
        self.input_token_usage, self.output_token_usage = 0, 0

    def write(
        self,
        topic: str,
        outline: str,
        review_id: Union[int, str, None],
        rag_num: int = 30,
        subsection_len: int = 500,
        refining: bool = True,
        save_dir: str = RAW_SAVEDIR,
        final_dir: str = REFINED_SAVEDIR,
        end_extension: str = "",
    ) -> Optional[str]:
        """
        Creates a subsection content draft and saves it.

        Parameters:
        topic - the topic of the survey to write.
        outline - the outline generated by the outline_writer.
        review_id - the ID of the original review paper used as the basis (for filtering references).
        rag_num - the number of documents to retrieve.
        subsection_len - the length of the subsections to write.
        refining - whether to refine the subsections.
        dataset - the dataset being used.
        end_extension - the extension to use for saving the file if needed for theSciReviewGen subsetting).

        Returns:
        the (potentially refined) survey with references if not written already, otherwise None (as it gets loaded again later on).
        """

        # First check if the files already exist
        parsed_outline = self.parse_outline(outline=outline)
        refined_name = os.path.join(final_dir, f"{review_id}{end_extension}.txt")
        out_name = os.path.join(save_dir, f"{review_id}_raw{end_extension}.txt")
        section_name = os.path.join(save_dir, f"{review_id}_sect{end_extension}.json")
        reference_name = os.path.join(final_dir, f"{review_id}_ref{end_extension}.json")
        if os.path.isfile(refined_name):

            print(
                f"Refined survey for article `{review_id}` already generated. Skipping..."
            )
            return None

        if os.path.isfile(out_name):
            print(
                f"Raw survey for article `{review_id}` already written. Skipping outline creation..."
            )
            raw_survey = load_txt(out_name, use_truncation=False)
            section_content = load_json(section_name)

            # Add length to token counter (note: this would not include how many tokens were used for the writing process)
            self.output_token_usage += self.token_counter.num_tokens_from_string(
                raw_survey
            )

        else:
            # Get the database
            section_content = [[]] * len(parsed_outline["sections"])
            section_paper_texts = [[]] * len(parsed_outline["sections"])
            section_references_ids = [[]] * len(parsed_outline["sections"])
            total_ids = []
            for i in range(len(parsed_outline["sections"])):

                descriptions = parsed_outline["subsection_descriptions"][i]
                for d in descriptions:

                    references_ids = self.db.get_ids_from_query(
                        d, review_id=review_id, num=rag_num
                    )
                    total_ids += references_ids
                    section_references_ids[i].append(references_ids)

            total_references_infos = self.db.get_paper_info_from_ids(
                list(set(total_ids))
            )
            temp_title_dict = {p["id"]: p["title"] for p in total_references_infos}
            temp_abs_dict = {p["id"]: p["abs"] for p in total_references_infos}

            for i in range(len(parsed_outline["sections"])):

                for references_ids in section_references_ids[i]:

                    references_titles = [temp_title_dict[_] for _ in references_ids]
                    references_papers = [temp_abs_dict[_] for _ in references_ids]
                    paper_texts = ""
                    for t, p in zip(references_titles, references_papers):

                        paper_texts += (
                            f"---\n\npaper_title: {t}\n\npaper_content:\n\n{p}\n"
                        )

                    paper_texts += "---\n"
                    section_paper_texts[i].append(paper_texts)

            thread_l = []
            for i in range(len(parsed_outline["sections"])):

                thread = threading.Thread(
                    target=self.write_subsection_with_reflection,
                    args=(
                        section_paper_texts[i],
                        topic,
                        outline,
                        parsed_outline["sections"][i],
                        parsed_outline["subsections"][i],
                        parsed_outline["subsection_descriptions"][i],
                        section_content,
                        i,
                        str(subsection_len),
                    ),
                )
                thread_l.append(thread)
                thread.start()
                time.sleep(0.1)

            for thread in thread_l:

                thread.join()

            raw_survey = self.generate_document(parsed_outline, section_content)

            # Get the references
            raw_survey_with_references, raw_references = self.process_references(
                raw_survey, review_id
            )

            # Save variables in case of connection/other issues
            os.makedirs(save_dir, exist_ok=True)
            save_to_json(section_content, section_name)

            # Memory Leak Prevention
            del raw_survey

        if refining:
            final_section_content = self.refine_subsections(
                topic, outline, section_content
            )
            refined_survey = self.generate_document(
                parsed_outline, final_section_content
            )

            # Update the file if refinement occurred
            refined_survey_with_references, references = self.process_references(
                refined_survey, review_id
            )

            # Save the results
            save_to_json(references, reference_name)

            # Memory Leak Prevention
            del references, refined_survey

            return refined_survey_with_references

        else:
            # Save the references
            save_to_json(raw_references, reference_name)

            # Memory Leak Prevention
            del raw_references

            return raw_survey_with_references

    def compute_price(self):

        return self.token_counter.compute_price(
            input_tokens=self.input_token_usage,
            output_tokens=self.output_token_usage,
            model=self.model,
        )

    def refine_subsections(
        self, topic: str, outline: str, section_content: List[str]
    ) -> List[str]:
        """
        Refines the subsections based on the topic, outline, and section content.

        Parameters:
        topic - the topic of the survey.
        outline - the outline to process.
        section_content - the section contents to use.

        Returns:
        the refined section contents as a list.
        """

        section_content_even = copy.deepcopy(section_content)
        thread_l = []
        for i in range(len(section_content)):

            for j in range(len(section_content[i])):

                if j % 2 == 0:
                    if j == 0:
                        contents = [""] + section_content[i][:2]

                    elif j == (len(section_content[i]) - 1):
                        contents = section_content[i][-2:] + [""]

                    else:
                        contents = section_content[i][j - 1 : j + 2]

                    thread = threading.Thread(
                        target=self.lce,
                        args=(topic, outline, contents, section_content_even[i], j),
                    )
                    thread_l.append(thread)
                    thread.start()

        for thread in thread_l:

            thread.join()

        final_section_content = copy.deepcopy(section_content_even)

        thread_l = []
        for i in range(len(section_content_even)):

            for j in range(len(section_content_even[i])):

                if j % 2 == 1:
                    if j == (len(section_content_even[i]) - 1):
                        contents = section_content_even[i][-2:] + [""]

                    else:
                        contents = section_content_even[i][j - 1 : j + 2]
                    thread = threading.Thread(
                        target=self.lce,
                        args=(topic, outline, contents, final_section_content[i], j),
                    )
                    thread_l.append(thread)
                    thread.start()

        for thread in thread_l:

            thread.join()

        return final_section_content

    def write_subsection_with_reflection(
        self,
        paper_texts_l: List[str],
        topic: str,
        outline: str,
        section: str,
        subsections: List[str],
        subdescriptions: List[str],
        res_l: List,
        idx: int,
        subsection_len: int = 1000,
        citation_num: int = 8,
    ) -> List[str]:
        """
        Writes the subsections with reflection (citation verification).

        Parameters:
        paper_texts_l - the list of paper texts.
        topic - the topic of the survey.
        outline - the generated survey outline.
        section - the name of the section.
        subsections - the list of subsections.
        subdescriptions - the list of descriptions for each subsection.
        res_l - the result list to save to.
        idx - the location to store the output in for `res_l`.
        subsection_len - the target length the subsections should reach (in words).
        citation_num - the number of citations in the subsection.

        Returns:
        the list of outputs from the model to select from.
        """

        prompts = []
        for j in range(len(subsections)):

            subsection, description = subsections[j], subdescriptions[j]
            prompt = self.__generate_prompt(
                SUBSECTION_WRITING_PROMPT,
                paras={
                    "OVERALL OUTLINE": outline,
                    "SUBSECTION NAME": subsection,
                    "DESCRIPTION": description,
                    "TOPIC": topic,
                    "PAPER LIST": paper_texts_l[j],
                    "SECTION NAME": section,
                    "WORD NUM": str(subsection_len),
                    "CITATION NUM": str(citation_num),
                },
            )
            prompts.append(prompt)

        self.input_token_usage += self.token_counter.num_tokens_from_list_string(
            prompts
        )
        contents = self.api_model.batch_chat(prompts, temperature=0.0)
        self.output_token_usage += self.token_counter.num_tokens_from_list_string(
            contents
        )
        contents = [
            c.replace("<format>", "").replace("</format>", "") for c in contents
        ]

        prompts = []
        for content, paper_texts in zip(contents, paper_texts_l):

            prompts.append(
                self.__generate_prompt(
                    CHECK_CITATION_PROMPT,
                    paras={
                        "SUBSECTION": content,
                        "TOPIC": topic,
                        "PAPER LIST": paper_texts,
                    },
                )
            )

        self.input_token_usage += self.token_counter.num_tokens_from_list_string(
            prompts
        )
        contents = self.api_model.batch_chat(prompts, temperature=0.0)
        self.output_token_usage += self.token_counter.num_tokens_from_list_string(
            contents
        )
        contents = [
            c.replace("<format>", "").replace("</format>", "") for c in contents
        ]

        res_l[idx] = contents
        return contents

    def __generate_prompt(self, template: str, paras: Dict) -> str:
        prompt = template
        for k in paras.keys():

            prompt = prompt.replace(f"[{k}]", paras[k])

        return prompt

    def lce(
        self, topic: str, outline: str, contents: List[str], res_l: List, idx: int
    ) -> str:
        """
        Refines the coherence of the generated contents.
        It is currently unclear what this acronym (lce) stands for, as it's not mentioned anywhere.
        However, one guess would be "Long/Logical Coherence Enhancement" based on the prompt contents.

        Parameters:
        topic - the topic of the survey.
        outline - the outline of the survey to be written.
        contents - the contents of each section.
        res_l - the result list to save the results to.
        idx - the location to store the output in for `res_l`.

        Returns:
        the refined subsection.
        """

        prompt = self.__generate_prompt(
            LCE_PROMPT,
            paras={
                "OVERALL OUTLINE": outline,
                "PREVIOUS": contents[0],
                "FOLLOWING": contents[2],
                "TOPIC": topic,
                "SUBSECTION": contents[1],
            },
        )
        self.input_token_usage += self.token_counter.num_tokens_from_string(prompt)
        refined_content = (
            self.api_model.chat(prompt, temperature=1)
            .replace("<format>", "")
            .replace("</format>", "")
        )
        self.output_token_usage += self.token_counter.num_tokens_from_string(
            refined_content
        )

        res_l[idx] = refined_content
        return refined_content.replace("Here is the refined subsection:\n", "")

    def parse_outline(self, outline: str) -> Dict:
        """
        Parses an outline into its parts (sections and subsections alongside the respective
        descriptions).

        Parameters:
        outline - the outline string to parse.

        Returns:
        the parsed outline with the following layout:

        {
            "title": "[SURVEY_TITLE]",
            "sections": [LIST_OF_SURVEY_SECTIONS],
            "section_descriptions": [LIST_OF_SURVEY_SECTION_DESCRIPTIONS],
            "subsections": [LIST_OF_SURVEY_SUBSECTIONS],
            "subsection_descriptions": [LIST_OF_SURVEY_SUBSECTION_DESCRIPTIONS],
        }
        """

        result = {
            "title": "",
            "sections": [],
            "section_descriptions": [],
            "subsections": [],
            "subsection_descriptions": [],
        }

        # Split the outline into lines
        lines = outline.split("\n")

        for i, line in enumerate(lines):
            # Match title, sections, subsections and their descriptions
            if line.startswith("# "):
                result["title"] = line[2:].strip()

            elif line.startswith("## "):
                result["sections"].append(line[3:].strip())
                # Extract the description in the next line
                if i + 1 < len(lines) and lines[i + 1].startswith("Description:"):
                    result["section_descriptions"].append(
                        lines[i + 1].split("Description:", 1)[1].strip()
                    )
                    result["subsections"].append([])
                    result["subsection_descriptions"].append([])

            elif line.startswith("### "):
                if result["subsections"]:
                    result["subsections"][-1].append(line[4:].strip())
                    # Extract the description in the next line
                    if i + 1 < len(lines) and lines[i + 1].startswith("Description:"):
                        result["subsection_descriptions"][-1].append(
                            lines[i + 1].split("Description:", 1)[1].strip()
                        )

        return result

    def process_references(
        self, survey: str, review_id: Union[int, str, None]
    ) -> Tuple[str, Dict]:
        """
        Identifies the citations in a survey and replaces them with numbers.

        Parameters:
        survey - the survey to adjust.
        review_id - the ID of the original review paper used as the basis (for filtering references).

        Returns:
        the cleaned survey.
        """

        citations = self.extract_citations(survey)
        return self.replace_citations_with_numbers(citations, survey, review_id)

    def generate_document(self, parsed_outline: Dict, subsection_contents: List) -> str:
        """
        Formats a parsed outline and contents into a text file.

        Parameters:
        parsed_outline - the parsed outline.
        subsection_contents - the contents of each subsection.

        Returns:
        the survey as a text.
        """

        document = []
        # Append the title
        title = parsed_outline["title"]
        document.append(f"{title}\n")

        # Iterate over the sections and their content
        for i, section in enumerate(parsed_outline["sections"]):

            document.append(f"# {section}\n")
            # Append the subsections and their contents
            for j, subsection in enumerate(parsed_outline["subsections"][i]):

                document.append(f"## {subsection}\n")

                # Append the detailed content for each subsection
                if i < len(subsection_contents) and j < len(subsection_contents[i]):
                    document.append(subsection_contents[i][j] + "\n")

        return "\n".join(document)

    def extract_citations(self, markdown_text: str) -> List[str]:
        """
        Extracts the citations from a text.

        Parameters:
        markdown_text - the markdown text to parse.

        Returns:
        the list of unique citations in the text, cleaned.
        """

        # Regular expression matches the content in square brackets
        pattern = re.compile(r"\[(.*?)\]")
        matches = pattern.findall(markdown_text)

        # Split references, handle multiple references, and remove duplicates
        citations = []
        for match in matches:

            # Split individual references and remove spaces
            parts = match.split(";")
            for part in parts:

                cit = part.strip()
                if cit not in citations:
                    citations.append(cit)

        return citations

    def replace_citations_with_numbers(
        self, citations: List[str], markdown_text: str, review_id: Union[int, str, None]
    ) -> Tuple[str, Dict]:
        """
        Replaces the citations in the survey with numbers.

        Parameters:
        citations - the list of citations.
        markdown_text - the formatted markdown survey.
        review_id - the ID of the original review paper used as the basis (for filtering references).

        Returns:
        the final survey (including references section) and the dictionary
        of references.
        """

        ids = self.db.get_titles_from_citations(citations, review_id)
        citation_to_ids = {citation: idx for citation, idx in zip(citations, ids)}

        paper_infos = self.db.get_paper_info_from_ids(ids)
        temp_dict = {p["id"]: p["title"] for p in paper_infos}
        titles = [temp_dict[_] for _ in tqdm(ids)]

        ids_to_titles = {idx: title for idx, title in zip(ids, titles)}
        titles_to_ids = {title: idx for idx, title in ids_to_titles.items()}

        title_to_number = {title: num + 1 for num, title in enumerate(titles)}
        title_to_number = {
            title: num + 1 for num, title in enumerate(title_to_number.keys())
        }

        number_to_title = {num: title for title, num in title_to_number.items()}
        number_to_title_sorted = {
            key: number_to_title[key] for key in sorted(number_to_title)
        }

        def replace_match(match):
            citation_text = match.group(1)
            individual_citations = citation_text.split(";")

            numbered_citations = [
                str(title_to_number[ids_to_titles[citation_to_ids[citation.strip()]]])
                for citation in individual_citations
            ]

            return "[" + "; ".join(numbered_citations) + "]"

        updated_text = re.sub(r"\[(.*?)\]", replace_match, markdown_text)
        references_section = "\n\n## References\n\n"
        references = {
            num: titles_to_ids[title] for num, title in number_to_title_sorted.items()
        }
        for idx, title in number_to_title_sorted.items():

            t = title.replace("\n", "")
            references_section += f"[{idx}] {t}\n\n"

        return updated_text + references_section, references

    # This was absent in the original code
    def extract_title_sections_descriptions(
        self, outline: str
    ) -> Tuple[str, List[str], List[str]]:
        """
        Parses an outline into its parts.

        Parameters:
        outline - the outline to parse.

        Returns:
        the title, sections, and descriptions of the outline.
        """

        title = outline.split("Title: ")[1].split("\n")[0]
        sections, descriptions = [], []
        for i in range(100):

            if f"Section {i+1}" in outline:
                sections.append(outline.split(f"Section {i+1}: ")[1].split("\n")[0])
                descriptions.append(
                    outline.split(f"Description {i+1}: ")[1].split("\n")[0]
                )

        return title, sections, descriptions
